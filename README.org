#+PROPERTY: header-args:R :results none

* ProbTheory.jl
A script aiming to visualize basic probability theory concepts.

#+begin_src sh :results none
julia -e "include(\"ProbTheory.jl\");
          LLN();
          CLT();
          BE_binom_heatmap(tight = false);
          BE_binom_heatmap(tight = true);
          BE_binom_slice()"
#+end_src

#+begin_comment
currently ess isn't working with org-babel

#+LATEX_CLASS: notes
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setmainfont[]{IBM Plex Sans}
#+LATEX_HEADER: \setmonofont[]{Iosevka SS14}
#+end_comment
* Law of Large Numbers (n = 600) [[[https://en.wikipedia.org/wiki/Law_of_large_numbers#Forms][Ref]]]
$$\bar{X}_n = \frac{1}{n} (X_1 + \cdots + X_n)$$

$$n \to \infty \Longrightarrow \bar{X_n} \to \mu$$

This is sampling from a Uniform(-100000, 100000), so $\mu = 0$

[[file:./media/LLN.gif]]

* Central Limit Theorem - Classical (n = 600)
[[file:./media/CLT.gif]]

* Berry-Esseen Theorem - Binomial Case [[[https://ubt.opus.hbz-nrw.de/opus45-ubtr/frontdoor/deliver/index/docId/732/file/Dissertation_Schulz.pdf][Ref (Thm 1)]]]
Theorem 1 Verbatim:

Let $p \in (0,1)$ and $n \in \mathbb{N}$ and let F_{n,p} denote the distribution function of the binomial distribution with parameters $n$ and $p$. Then we have with $q := 1 - p$

$$sup_{x \in \mathbb{R}}{\left|F_{n,p}(x) - \Phi\left(\frac{x - np}{\sqrt{npq}}\right) \right| < \frac{\sqrt{10} + 3}{6\sqrt{2\pi}} \cdot \frac{p^2 + q^2}{\sqrt{npq}} }$$

In the case $p \in \left[\frac{1}{3}, \frac{2}{3}\right]$ we even have the sharper inequality

$$sup_{x \in \mathbb{R}}{\left|F_{n,p}(x) - \Phi\left(\frac{x - np}{\sqrt{npq}}\right) \right| < \frac{3 + |p - q|}{6\sqrt{2\pi}\sqrt{npq}} }$$

----------

This is a specific version of the [[https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem#cite_note-1][Berry-Esseen Theorem]], which states that the distribution of the sample means from a specific subset of random variables will converge to the standard normal, by an order of $\frac{1}{\sqrt{n}}$.

This Binomial variant gives two bounds, the first being general for all values of p, and the second being tighter but only for the middle third range.

The next two plots depict the "error" term between the Binomial and Standard Normal. Essentially, they numerically back Schulz' Theorem.

#+begin_comment
NOTE
When writing about BE,
Need to show what ksdistance is
by plotting the two cdfs and line between
for n = 5, 10, 50, ...
#+end_comment

Binomial BE Heatmap (n = 500, tight = false)
[[file:./media/BE_binom_heatmap_500_regbound.png]]

Binomial BE Heatmap (n = 500, tight = true)
[[file:./media/BE_binom_heatmap_500_tightbound.png]]

Binomial BE Slice (n = 20, tight = true)
[[file:./media/BE_binom_slice.png]]

We can also check that none of the tight errors are negative in the middle third, but are in the outer two thirds:

#+begin_src sh :results output raw
julia -e "include(\"ProbTheory.jl\"); BE_binom_bounds(1000)"
#+end_src

For n = 1000 on p = [0.001, 0.999], there are 183 negative differences.

For n = 1000 on p = [0.333, 0.666], there are 0 negative differences.

* Appendix
** Reference
$$E[X] = \int_{-\infty}^{\infty} xf(x)dx$$

Using a Uniform Distribution:

$$f(x) = \frac{1}{b-a}$$

** Expectations
$$E[X] = \int_a^b x \frac{1}{b-a} = \frac{1}{2(b-a)} [x^2]_a^b = \frac{a+b}{2}$$

$$E[X^2] = \int_a^b x^2 \frac{1}{b-a} = \frac{1}{3(b-a)} [x^3]_a^b = \frac{(b-a)(b^2 + ba + a^2)}{3(b-a)} = \frac{b^2 + ba + a^2}{3}$$

** Checking the Variance
$$Var(X) = E[X^2] - E[X]^2$$

$$Var(X) = \frac{b^2 + ba + a^2}{3} - (\frac{a+b}{2})^2$$

$$Var(X) = \frac{b^2 + ba + a^2}{3} - \frac{a^2+2ab+b^2}{4}$$

$$Var(X) = \frac{4b^2 + 4ba + 4a^2}{12} - \frac{3a^2+6ab+3b^2}{12}$$

$$Var(X) = \frac{b^2 - 2ba + a^2}{12} = \frac{(b-a)^2}{12}$$

** E[|X|^3]
$$E[|X|^3] = \int_a^b |x|^3 \frac{1}{b-a}$$

$$ = \frac{1}{b-a} (\int_a^0 -x^3  + \int_0^b x^3)$$

$$ = \frac{1}{b-a} (-\frac{1}{4}[x^4]_a^0  + \frac{1}{4}[x^4]_0^b)$$

$$ = \frac{1}{b-a} (\frac{1}{4}a^4  + \frac{1}{4}b^4)$$

$$ = \frac{1}{4} \frac{1}{b-a} (a^4  + b^4)$$
